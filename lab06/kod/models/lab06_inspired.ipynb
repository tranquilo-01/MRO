{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Augmentacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "cifar_trainset = CIFAR10(root='./data', train=True, download=False)\n",
    "data = cifar_trainset.data / 255\n",
    "\n",
    "mean = data.mean(axis=(0, 1, 2))\n",
    "std = data.std(axis=(0, 1, 2))\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomResizedCrop(size=32, scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2, 3. Przypięcie augmentacji, załadowanie CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CIFAR10(root='./data', train=True,\n",
    "                        transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64,\n",
    "                          shuffle=True, num_workers=4)\n",
    "\n",
    "test_dataset = CIFAR10(root='./data', train=False,\n",
    "                       transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64,\n",
    "                         shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4, 5, 6. Patching i model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, hidden_dim, dropout_rate=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embed_size, num_heads=num_heads)\n",
    "        self.linear1 = nn.Linear(embed_size, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, embed_size)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.norm1(x)\n",
    "        y, _ = self.multihead_attn(y, y, y)\n",
    "        y = self.dropout(y)\n",
    "        x = x + y\n",
    "\n",
    "        y = self.norm2(x)\n",
    "        y = self.linear1(y)\n",
    "        y = self.activation(y)\n",
    "        y = self.dropout(y)\n",
    "        y = self.linear2(y)\n",
    "        y = self.dropout(y)\n",
    "        x = x + y\n",
    "\n",
    "        return x\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_channels, embed_size):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        self.linear = nn.Linear(patch_size * patch_size * in_channels, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        num_patches_h = self.img_size // self.patch_size\n",
    "        num_patches_w = self.img_size // self.patch_size\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = x.view(x.shape[0], num_patches_h, self.patch_size, num_patches_w, self.patch_size, x.shape[3])\n",
    "        x = x.permute(0, 1, 3, 2, 4, 5).contiguous()\n",
    "        x = x.view(x.shape[0], -1, self.patch_size * self.patch_size * x.shape[-1])\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_channels, embed_size, num_heads, depth, n_classes):\n",
    "        super().__init__()\n",
    "        self.patch_embedding = PatchEmbedding(img_size, patch_size, in_channels, embed_size)\n",
    "\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, self.patch_embedding.n_patches, embed_size))\n",
    "\n",
    "        transformers = [\n",
    "            TransformerEncoderLayer(\n",
    "                embed_size=embed_size, \n",
    "                num_heads=num_heads, \n",
    "                hidden_dim=512\n",
    "            )\n",
    "        ] * depth\n",
    "        \n",
    "        self.transformers = nn.Sequential(*transformers)\n",
    "\n",
    "        self.linear = nn.Linear(embed_size, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.patch_embedding(x)\n",
    "        x += self.positional_encoding\n",
    "        x = self.transformers(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, lr: 0.002, loss/train: 1.7006267143027556, loss/test: 1.5123579972868513, acc/train: 0.37496, acc/test: 0.4315\n",
      "epoch: 1, lr: 0.002, loss/train: 1.4076622809900348, loss/test: 1.350803022931336, acc/train: 0.49014, acc/test: 0.5057\n",
      "epoch: 2, lr: 0.002, loss/train: 1.2864463456603876, loss/test: 1.298111631991757, acc/train: 0.53732, acc/test: 0.53\n",
      "epoch: 3, lr: 0.002, loss/train: 1.1975068883670261, loss/test: 1.1641122824067522, acc/train: 0.57464, acc/test: 0.5901\n",
      "epoch: 4, lr: 0.002, loss/train: 1.1298167079001131, loss/test: 1.0969695452671901, acc/train: 0.59814, acc/test: 0.6023\n",
      "epoch: 5, lr: 0.002, loss/train: 1.088044859564213, loss/test: 1.1205699360294707, acc/train: 0.61692, acc/test: 0.6048\n",
      "epoch: 6, lr: 0.002, loss/train: 1.0571758250903596, loss/test: 1.0525666744845688, acc/train: 0.62754, acc/test: 0.6222\n",
      "epoch: 7, lr: 0.002, loss/train: 1.0315475216149674, loss/test: 1.0242400788197852, acc/train: 0.63498, acc/test: 0.6387\n",
      "epoch: 8, lr: 0.002, loss/train: 1.0118936166129149, loss/test: 1.0099659395066036, acc/train: 0.64428, acc/test: 0.6426\n",
      "epoch: 9, lr: 0.002, loss/train: 0.9906731292109965, loss/test: 1.0163020070191402, acc/train: 0.65032, acc/test: 0.6374\n",
      "epoch: 10, lr: 0.002, loss/train: 0.9752137358383755, loss/test: 0.9975218370461919, acc/train: 0.6562, acc/test: 0.6447\n",
      "epoch: 11, lr: 0.002, loss/train: 0.9637342695994755, loss/test: 0.9888264590008243, acc/train: 0.65972, acc/test: 0.6462\n",
      "epoch: 12, lr: 0.002, loss/train: 0.9468654901017923, loss/test: 0.9716035321259954, acc/train: 0.66648, acc/test: 0.6578\n",
      "epoch: 13, lr: 0.002, loss/train: 0.9352256032969336, loss/test: 0.9485859491263225, acc/train: 0.67136, acc/test: 0.6657\n",
      "epoch: 14, lr: 0.002, loss/train: 0.9210211792412926, loss/test: 0.9741312011032347, acc/train: 0.67746, acc/test: 0.6625\n",
      "epoch: 15, lr: 0.002, loss/train: 0.9089631674539708, loss/test: 0.9427965776935504, acc/train: 0.68174, acc/test: 0.668\n",
      "epoch: 16, lr: 0.002, loss/train: 0.8980490274898841, loss/test: 0.9185938664302704, acc/train: 0.68458, acc/test: 0.6781\n",
      "epoch: 17, lr: 0.002, loss/train: 0.8878286404301748, loss/test: 0.9142858670775298, acc/train: 0.688, acc/test: 0.6799\n",
      "epoch: 18, lr: 0.002, loss/train: 0.8791910436223535, loss/test: 0.9165851583906041, acc/train: 0.69092, acc/test: 0.6837\n",
      "epoch: 19, lr: 0.002, loss/train: 0.86092626316773, loss/test: 0.8831559704367522, acc/train: 0.6967, acc/test: 0.6946\n",
      "epoch: 20, lr: 0.002, loss/train: 0.8543941374020199, loss/test: 0.8964711161935406, acc/train: 0.70132, acc/test: 0.6902\n",
      "epoch: 21, lr: 0.002, loss/train: 0.8418530637727064, loss/test: 0.8961696860137259, acc/train: 0.70618, acc/test: 0.6813\n",
      "epoch: 22, lr: 0.002, loss/train: 0.8355324102560883, loss/test: 0.855604575318136, acc/train: 0.70558, acc/test: 0.7057\n",
      "epoch: 23, lr: 0.002, loss/train: 0.829572989279047, loss/test: 0.892431230681717, acc/train: 0.71092, acc/test: 0.6924\n",
      "epoch: 24, lr: 0.002, loss/train: 0.81968359809245, loss/test: 0.8580323057189868, acc/train: 0.7128, acc/test: 0.7052\n",
      "epoch: 25, lr: 0.002, loss/train: 0.8102680903185359, loss/test: 0.8497626519506905, acc/train: 0.71254, acc/test: 0.7074\n",
      "epoch: 26, lr: 0.002, loss/train: 0.8027836194504863, loss/test: 0.8648502576123377, acc/train: 0.71922, acc/test: 0.6983\n",
      "epoch: 27, lr: 0.002, loss/train: 0.8025782421574263, loss/test: 0.8365532366713141, acc/train: 0.71976, acc/test: 0.7087\n",
      "epoch: 28, lr: 0.002, loss/train: 0.7983524633185638, loss/test: 0.8613705638867275, acc/train: 0.72282, acc/test: 0.705\n",
      "epoch: 29, lr: 0.002, loss/train: 0.7842971966852008, loss/test: 0.8235871865870846, acc/train: 0.72374, acc/test: 0.7124\n",
      "epoch: 30, lr: 0.002, loss/train: 0.784322656252805, loss/test: 0.8164575572606105, acc/train: 0.72514, acc/test: 0.7148\n",
      "epoch: 31, lr: 0.002, loss/train: 0.7818988289903192, loss/test: 0.8231843347382394, acc/train: 0.72446, acc/test: 0.7167\n",
      "epoch: 32, lr: 0.002, loss/train: 0.7722722521752042, loss/test: 0.8369677522379881, acc/train: 0.73126, acc/test: 0.7107\n",
      "epoch: 33, lr: 0.002, loss/train: 0.7704364671502881, loss/test: 0.8315136037814389, acc/train: 0.73148, acc/test: 0.7109\n",
      "epoch: 34, lr: 0.002, loss/train: 0.771418980396617, loss/test: 0.8235878160425053, acc/train: 0.73094, acc/test: 0.7195\n",
      "LR drop\n",
      "epoch: 35, lr: 0.0002, loss/train: 0.7671814596713962, loss/test: 0.7998161687972439, acc/train: 0.73252, acc/test: 0.7201\n",
      "epoch: 36, lr: 0.0002, loss/train: 0.6533074376680662, loss/test: 0.7230119671031927, acc/train: 0.77218, acc/test: 0.7513\n",
      "epoch: 37, lr: 0.0002, loss/train: 0.6275435821022219, loss/test: 0.7144205572119184, acc/train: 0.78314, acc/test: 0.7503\n",
      "epoch: 38, lr: 0.0002, loss/train: 0.6175547453677258, loss/test: 0.7103567163276064, acc/train: 0.78642, acc/test: 0.7569\n",
      "epoch: 39, lr: 0.0002, loss/train: 0.6123873434027137, loss/test: 0.7052888363409954, acc/train: 0.78474, acc/test: 0.7563\n",
      "epoch: 40, lr: 0.0002, loss/train: 0.6020089215253626, loss/test: 0.694730141360289, acc/train: 0.7888, acc/test: 0.7599\n",
      "epoch: 41, lr: 0.0002, loss/train: 0.6021898539779741, loss/test: 0.698006372922545, acc/train: 0.78706, acc/test: 0.7611\n",
      "epoch: 42, lr: 0.0002, loss/train: 0.5925219925239568, loss/test: 0.6905241460557197, acc/train: 0.79198, acc/test: 0.7662\n",
      "epoch: 43, lr: 0.0002, loss/train: 0.5867971109078668, loss/test: 0.7047152654000908, acc/train: 0.79342, acc/test: 0.7574\n",
      "epoch: 44, lr: 0.0002, loss/train: 0.5872430725170829, loss/test: 0.6948204860565769, acc/train: 0.79334, acc/test: 0.7651\n",
      "LR drop\n",
      "epoch: 45, lr: 2e-05, loss/train: 0.5846195684369567, loss/test: 0.702056882108093, acc/train: 0.79584, acc/test: 0.7572\n",
      "epoch: 46, lr: 2e-05, loss/train: 0.5684227278013059, loss/test: 0.6984881449277234, acc/train: 0.79992, acc/test: 0.7677\n",
      "epoch: 47, lr: 2e-05, loss/train: 0.567043373003945, loss/test: 0.6904893568746603, acc/train: 0.80114, acc/test: 0.7668\n",
      "epoch: 48, lr: 2e-05, loss/train: 0.5632830584598014, loss/test: 0.6889147591439022, acc/train: 0.80304, acc/test: 0.7634\n",
      "epoch: 49, lr: 2e-05, loss/train: 0.5631203550054594, loss/test: 0.6975803790958064, acc/train: 0.8034, acc/test: 0.7607\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = ViT(\n",
    "    img_size=32, \n",
    "    patch_size=4,    \n",
    "    in_channels=3, \n",
    "    embed_size=256, \n",
    "    num_heads=8, \n",
    "    depth=6,\n",
    "    n_classes=10\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.002)\n",
    "num_epochs = 50\n",
    "lr_drop_epochs = [35, 45]\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss, train_correct = 0.0, 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    test_loss, test_correct = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "    if epoch in lr_drop_epochs:\n",
    "        print(\"LR drop\")\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] /= 10\n",
    "\n",
    "    logs = {\n",
    "        \"epoch\": epoch,\n",
    "        \"lr\": optimizer.param_groups[0]['lr'], \n",
    "        \"loss/train\": train_loss / len(train_loader),\n",
    "        \"loss/test\": test_loss / len(test_loader),\n",
    "        \"acc/train\": train_correct / len(train_dataset),\n",
    "        \"acc/test\": test_correct / len(test_dataset),\n",
    "    }\n",
    "\n",
    "    if logs[\"loss/test\"] < best_test_loss:\n",
    "        best_test_loss = logs[\"loss/test\"]\n",
    "        model_path = f\"best_model_epoch_{epoch:03d}_test_loss{best_test_loss:.1e}.pth\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        \n",
    "    print(\", \".join([f\"{k}: {v:}\" for k, v in logs.items()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchviz import make_dot\n",
    "\n",
    "# # Instantiate the model\n",
    "# # Dummy input tensor (batch size 1, 3 channels, 32x32 image)\n",
    "# dummy_input = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "# # Forward pass and graph visualization\n",
    "# output = model(dummy_input)\n",
    "# graph = make_dot(output, params=dict(model.named_parameters()))\n",
    "\n",
    "# # Save or display the graph\n",
    "# graph.render(\"visual_transformer_graph\", format=\"png\", cleanup=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mro_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
