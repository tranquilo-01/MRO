{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Augmentacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "cifar_trainset = CIFAR10(root='./data', train=True, download=False)\n",
    "data = cifar_trainset.data / 255\n",
    "\n",
    "mean = data.mean(axis=(0, 1, 2))\n",
    "std = data.std(axis=(0, 1, 2))\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomResizedCrop(size=32, scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2, 3 - przypięcie augmentacji, załadowanie cifar-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset = CIFAR10(root='./data', train=True,\n",
    "                        transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64,\n",
    "                          shuffle=True, num_workers=4)\n",
    "\n",
    "test_dataset = CIFAR10(root='./data', train=False,\n",
    "                       transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64,\n",
    "                         shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4, 5 - Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def image_to_patches(images, patch_size):\n",
    "    batch_size, channels, height, width = images.size()\n",
    "    assert height % patch_size == 0 and width % patch_size == 0, \"Image size must be divisible by patch size\"\n",
    "\n",
    "    patches = images.unfold(2, patch_size, patch_size).unfold(\n",
    "        3, patch_size, patch_size)\n",
    "    patches = patches.contiguous().view(\n",
    "        batch_size, channels, -1, patch_size, patch_size)\n",
    "    # (batch_size, num_patches, channels, patch_size, patch_size)\n",
    "    patches = patches.permute(0, 2, 1, 3, 4)\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(train_loader)\n",
    "images, _ = next(data_iter)\n",
    "patches = image_to_patches(images, patch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_iter = iter(train_loader)\n",
    "images, _ = next(data_iter)\n",
    "patches = image_to_patches(images, patch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_patches(img):\n",
    "    fig, axs = plt.subplots(8, 8, figsize=(8, 8))\n",
    "    for i, ax in enumerate(axs.flatten()):\n",
    "        patch = img[i].permute(1, 2, 0)\n",
    "        ax.imshow((patch * torch.tensor(std) + torch.tensor(mean)).numpy())\n",
    "        ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 64 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_patches(patches[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomTransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.norm1(x)\n",
    "        attn_output, _ = self.attention(y, y, y)\n",
    "        x = x + attn_output\n",
    "        \n",
    "        x = x + self.mlp(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisualTransformer(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, embed_dim=256, num_classes=10, depth=6, num_heads=8, mlp_dim=512, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # Patch Embedding\n",
    "        self.patch_embedding = nn.Linear(patch_size * patch_size * 3, embed_dim)\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n",
    "\n",
    "        # Class Token\n",
    "        self.class_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "\n",
    "        # Dropout\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.transformer = nn.ModuleList([\n",
    "            CustomTransformerEncoderBlock(embed_dim, num_heads, mlp_dim, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        # Output layers\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        self.mlp_head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Divide image into patches\n",
    "        patches = image_to_patches(x, self.patch_size).flatten(2)\n",
    "        embeddings = self.patch_embedding(patches)\n",
    "\n",
    "        # Add class token and positional embeddings\n",
    "        cls_token = self.class_token.expand(batch_size, -1, -1)\n",
    "        embeddings = torch.cat([cls_token, embeddings], dim=1) + self.positional_embedding\n",
    "\n",
    "        # Apply dropout after embeddings\n",
    "        embeddings = self.embedding_dropout(embeddings)\n",
    "\n",
    "        # Pass through Transformer layers\n",
    "        for layer in self.transformer:\n",
    "            embeddings = layer(embeddings)\n",
    "\n",
    "        # Classification based on the class token\n",
    "        cls_output = self.layer_norm(embeddings[:, 0])\n",
    "        return self.mlp_head(cls_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'visual_transformer_graph.png'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from torchviz import make_dot\n",
    "\n",
    "# # Instantiate the model\n",
    "# model = VisualTransformer(img_size=32, patch_size=4, embed_dim=256, num_classes=10)\n",
    "\n",
    "# # Dummy input tensor (batch size 1, 3 channels, 32x32 image)\n",
    "# dummy_input = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "# # Forward pass and graph visualization\n",
    "# output = model(dummy_input)\n",
    "# graph = make_dot(output, params=dict(model.named_parameters()))\n",
    "\n",
    "# # Save or display the graph\n",
    "# graph.render(\"visual_transformer_graph\", format=\"png\", cleanup=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch,lr,train_loss,test_loss,train_acc,test_acc,epoch_time,total_time\n"
     ]
    }
   ],
   "source": [
    "from torch import inf\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = VisualTransformer().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=0.002)\n",
    "scheduler = MultiStepLR(optimizer, milestones=[30, 52], gamma=0.1)\n",
    "\n",
    "training_start_time = datetime.now()\n",
    "save_path = f\"./models/vit_{training_start_time:.0f}.pth\"\n",
    "best_eval_loss = inf\n",
    "print(\"epoch,lr,train_loss,test_loss,train_acc,test_acc,epoch_time,total_time\")\n",
    "for epoch in range(60):\n",
    "    epoch_start_time = datetime.now()\n",
    "    model.train()\n",
    "    train_loss, train_correct = 0.0, 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    test_loss, test_correct = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    if(test_loss < best_eval_loss):\n",
    "        best_eval_loss = test_loss\n",
    "        torch.save(model.state_dict(), save_path)          \n",
    "\n",
    "    \n",
    "    scheduler.step()\n",
    "    print(f\"{epoch},{optimizer.param_groups[0]['lr']:.5f},{train_loss/len(train_loader):.4f},{test_loss/len(test_loader):.4f},{train_correct / len(train_dataset):.4f},{test_correct / len(test_dataset):.4f},{datetime.now() - epoch_start_time:.0f},{datetime.now() - training_start_time:.0f}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
