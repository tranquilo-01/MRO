{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Augmentacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "cifar_trainset = CIFAR10(root='./data', train=True, download=False)\n",
    "data = cifar_trainset.data / 255\n",
    "\n",
    "mean = data.mean(axis=(0, 1, 2))\n",
    "std = data.std(axis=(0, 1, 2))\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomResizedCrop(size=32, scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2, 3 - przypięcie augmentacji, załadowanie cifar-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset = CIFAR10(root='./data', train=True,\n",
    "                        transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64,\n",
    "                          shuffle=True, num_workers=4)\n",
    "\n",
    "test_dataset = CIFAR10(root='./data', train=False,\n",
    "                       transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64,\n",
    "                         shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4, 5 - Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def image_to_patches(images, patch_size):\n",
    "        _, _, height, width = images.size()\n",
    "        num_patches_h = height // patch_size\n",
    "        num_patches_w = width // patch_size\n",
    "        x = images\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = x.view(x.shape[0], num_patches_h, patch_size, num_patches_w, patch_size, x.shape[3])\n",
    "        x = x.permute(0, 1, 3, 2, 4, 5).contiguous()\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_iter = iter(train_loader)\n",
    "images, _ = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = image_to_patches(images, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def plot_patches_grid(patches):\n",
    "    num_patches_h, num_patches_w, _, _, _ = patches.shape\n",
    "    \n",
    "    fig, axes = plt.subplots(num_patches_h, num_patches_w, figsize=(8, 8))\n",
    "    for i in range(num_patches_h):\n",
    "        for j in range(num_patches_w):\n",
    "            patch = patches[i, j]\n",
    "            patch = (patch * torch.tensor(std) + torch.tensor(mean))\n",
    "            patch = torch.clamp(patch, 0, 1)\n",
    "            axes[i, j].imshow(patch.numpy())\n",
    "            axes[i, j].axis(\"off\")\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 64 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_patches_grid(patches[21])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.norm1(x)\n",
    "        attn_output, _ = self.attention(y, y, y)\n",
    "        x = x + attn_output\n",
    "        \n",
    "        x = x + self.mlp(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisualTransformer(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, embed_dim=256, num_classes=10, depth=6, num_heads=8, mlp_dim=512, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # Patch Embedding\n",
    "        self.patch_embedding = nn.Linear(patch_size * patch_size * 3, embed_dim)\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n",
    "\n",
    "        # Class Token\n",
    "        self.class_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "\n",
    "        # Dropout\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.transformers = nn.ModuleList([\n",
    "            Transformer(embed_dim, num_heads, mlp_dim, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        # Output layers\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        self.mlp_head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # patch embedding\n",
    "        x = image_to_patches(x, self.patch_size)\n",
    "        x = x.view(x.shape[0], -1, self.patch_size * self.patch_size * x.shape[-1])\n",
    "        embeddings = self.patch_embedding(x)\n",
    "\n",
    "        # class token and positional embeddings\n",
    "        cls_token = self.class_token.expand(batch_size, -1, -1)\n",
    "        embeddings = torch.cat([cls_token, embeddings], dim=1) + self.positional_embedding\n",
    "\n",
    "        # dropout after embeddings\n",
    "        embeddings = self.embedding_dropout(embeddings)\n",
    "\n",
    "        # Pass through Transformer layers\n",
    "        for transformer in self.transformers:\n",
    "            embeddings = transformer(embeddings)\n",
    "\n",
    "        # Classification based on the class token\n",
    "        cls_output = self.layer_norm(embeddings[:, 0])\n",
    "        return self.mlp_head(cls_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchviz import make_dot\n",
    "\n",
    "# # Instantiate the model\n",
    "# model = VisualTransformer(img_size=32, patch_size=4, embed_dim=256, num_classes=10)\n",
    "\n",
    "# # Dummy input tensor (batch size 1, 3 channels, 32x32 image)\n",
    "# dummy_input = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "# # Forward pass and graph visualization\n",
    "# output = model(dummy_input)\n",
    "# graph = make_dot(output, params=dict(model.named_parameters()))\n",
    "\n",
    "# # Save or display the graph\n",
    "# graph.render(\"visual_transformer_graph\", format=\"png\", cleanup=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_timedelta(td):\n",
    "    total_seconds = int(td.total_seconds())\n",
    "    hours, remainder = divmod(total_seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return f\"{hours:02d}h:{minutes:02d}m:{seconds:02d}s\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch,lr,train_loss,test_loss,train_acc,test_acc,epoch_time,total_time\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[158], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 25\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     27\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\krzys\\anaconda3\\envs\\mro_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\krzys\\anaconda3\\envs\\mro_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[155], line 70\u001b[0m, in \u001b[0;36mVisualTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Pass through Transformer layers\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m transformer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformers:\n\u001b[1;32m---> 70\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Classification based on the class token\u001b[39;00m\n\u001b[0;32m     73\u001b[0m cls_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(embeddings[:, \u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\krzys\\anaconda3\\envs\\mro_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\krzys\\anaconda3\\envs\\mro_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[155], line 20\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     19\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)\n\u001b[1;32m---> 20\u001b[0m     attn_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m attn_output\n\u001b[0;32m     23\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(x)\n",
      "File \u001b[1;32mc:\\Users\\krzys\\anaconda3\\envs\\mro_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\krzys\\anaconda3\\envs\\mro_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\krzys\\anaconda3\\envs\\mro_env\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1266\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   1252\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1253\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[0;32m   1254\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1263\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[0;32m   1264\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[0;32m   1265\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1266\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1268\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1269\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1270\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[0;32m   1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32mc:\\Users\\krzys\\anaconda3\\envs\\mro_env\\Lib\\site-packages\\torch\\nn\\functional.py:5470\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   5468\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5469\u001b[0m     attn_output_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(q_scaled, k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m-> 5470\u001b[0m attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dropout_p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m   5472\u001b[0m     attn_output_weights \u001b[38;5;241m=\u001b[39m dropout(attn_output_weights, p\u001b[38;5;241m=\u001b[39mdropout_p)\n",
      "File \u001b[1;32mc:\\Users\\krzys\\anaconda3\\envs\\mro_env\\Lib\\site-packages\\torch\\nn\\functional.py:1885\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1883\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1885\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1887\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch import inf\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = VisualTransformer().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=0.002)\n",
    "scheduler = MultiStepLR(optimizer, milestones=[30, 52], gamma=0.1)\n",
    "\n",
    "training_start_time = datetime.now()\n",
    "save_path = f\"./models/vit_{int(training_start_time.timestamp())}.pth\"\n",
    "best_eval_loss = inf\n",
    "print(\"epoch,lr,train_loss,test_loss,train_acc,test_acc,epoch_time,total_time\")\n",
    "for epoch in range(60):\n",
    "    epoch_start_time = datetime.now()\n",
    "    model.train()\n",
    "    train_loss, train_correct = 0.0, 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    test_loss, test_correct = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    if(test_loss < best_eval_loss):\n",
    "        best_eval_loss = test_loss\n",
    "        torch.save(model.state_dict(), save_path)          \n",
    "\n",
    "    \n",
    "    scheduler.step()\n",
    "    print(f\"{epoch},{optimizer.param_groups[0]['lr']:.5f},{train_loss/len(train_loader):.4f},{test_loss/len(test_loader):.4f},{train_correct / len(train_dataset):.4f},{test_correct / len(test_dataset):.4f},{format_timedelta(datetime.now() - epoch_start_time)},{format_timedelta(datetime.now() - training_start_time)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mro_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
